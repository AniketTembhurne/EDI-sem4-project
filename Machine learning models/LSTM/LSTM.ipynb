{"cells":[{"cell_type":"markdown","metadata":{"id":"ePoUkERg8HpR"},"source":["# `SENTIMENTAL ANALYSIS`"]},{"cell_type":"markdown","metadata":{"id":"QXl6zLTm8HpU"},"source":["#### `IMPORTIN LIBRARIES`"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1650258543867,"user":{"displayName":"Ganesh Shinde","userId":"04780742815385189197"},"user_tz":-330},"id":"hFs4rEpz8HpV"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.optimizers import SGD"]},{"cell_type":"markdown","metadata":{"id":"cwJdfYrs8HpX"},"source":["### `IMPORTING DATASET`"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>0</td>\n","      <td>ate @user isz that youuu?ðððððð...</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>0</td>\n","      <td>to see nina turner on the airwaves trying to...</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>0</td>\n","      <td>listening to sad songs on a monday morning otw...</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>1</td>\n","      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>0</td>\n","      <td>thank you @user for you follow</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 2 columns</p>\n","</div>"],"text/plain":["       label                                              tweet\n","0          0   @user when a father is dysfunctional and is s...\n","1          0  @user @user thanks for #lyft credit i can't us...\n","2          0                                bihday your majesty\n","3          0  #model   i love u take with u all the time in ...\n","4          0             factsguide: society now    #motivation\n","...      ...                                                ...\n","31957      0  ate @user isz that youuu?ðððððð...\n","31958      0    to see nina turner on the airwaves trying to...\n","31959      0  listening to sad songs on a monday morning otw...\n","31960      1  @user #sikh #temple vandalised in in #calgary,...\n","31961      0                   thank you @user for you follow  \n","\n","[31962 rows x 2 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["dataset=pd.read_csv(\"train.csv\")\n","dataset.drop([\"id\"],axis=1)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  label                                              tweet\n","0   1      0   @user when a father is dysfunctional and is s...\n","1   2      0  @user @user thanks for #lyft credit i can't us...\n","2   3      0                                bihday your majesty\n","3   4      0  #model   i love u take with u all the time in ...\n","4   5      0             factsguide: society now    #motivation"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head()"]},{"cell_type":"markdown","metadata":{"id":"Cc_iEAkH8HpY"},"source":["### `PRE-PROCESSING`"]},{"cell_type":"markdown","metadata":{"id":"uw8hfhNc9JmN"},"source":["##### `PRE PROCESSING VARIABLE SETTING`"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# def clean_text(text):\n","#     '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n","#     and remove words containing numbers.'''\n","#     text = str(text).lower()\n","#     text = re.sub('\\[.*?\\]', ' ', text)\n","#     text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n","#     text = re.sub('<.*?>+', ' ', text)\n","#     text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n","#     text = re.sub('\\n', ' ', text)\n","#     text = re.sub('\\w*\\d\\w*', ' ', text)\n","#     return text"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["dataset['tweet'] = dataset['tweet'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>user father dysfunctional selfish drags kids d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>user user thanks lyft credit cant use cause do...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>model love u take u time ur</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>31958</td>\n","      <td>0</td>\n","      <td>ate user isz youuu</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>31959</td>\n","      <td>0</td>\n","      <td>see nina turner airwaves trying wrap mantle ge...</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>31960</td>\n","      <td>0</td>\n","      <td>listening sad songs monday morning otw work sad</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>31961</td>\n","      <td>1</td>\n","      <td>user sikh temple vandalised calgary wso condem...</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>31962</td>\n","      <td>0</td>\n","      <td>thank user follow</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 3 columns</p>\n","</div>"],"text/plain":["          id  label                                              tweet\n","0          1      0  user father dysfunctional selfish drags kids d...\n","1          2      0  user user thanks lyft credit cant use cause do...\n","2          3      0                                     bihday majesty\n","3          4      0                        model love u take u time ur\n","4          5      0                      factsguide society motivation\n","...      ...    ...                                                ...\n","31957  31958      0                                 ate user isz youuu\n","31958  31959      0  see nina turner airwaves trying wrap mantle ge...\n","31959  31960      0    listening sad songs monday morning otw work sad\n","31960  31961      1  user sikh temple vandalised calgary wso condem...\n","31961  31962      0                                  thank user follow\n","\n","[31962 rows x 3 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"tweet\"]=dataset[\"tweet\"].apply(lambda x: \" \".join([w for w in x.split() if w not in stopwords.words(\"english\")]))\n","dataset"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# Stemming\n","st=PorterStemmer()\n","dataset[\"tweet\"]=dataset['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>user father dysfunct selfish drag kid dysfunct...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>user user thank lyft credit cant use caus dont...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday majesti</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>model love u take u time ur</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguid societi motiv</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>31958</td>\n","      <td>0</td>\n","      <td>ate user isz youuu</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>31959</td>\n","      <td>0</td>\n","      <td>see nina turner airwav tri wrap mantl genuin h...</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>31960</td>\n","      <td>0</td>\n","      <td>listen sad song monday morn otw work sad</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>31961</td>\n","      <td>1</td>\n","      <td>user sikh templ vandalis calgari wso condemn act</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>31962</td>\n","      <td>0</td>\n","      <td>thank user follow</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 3 columns</p>\n","</div>"],"text/plain":["          id  label                                              tweet\n","0          1      0  user father dysfunct selfish drag kid dysfunct...\n","1          2      0  user user thank lyft credit cant use caus dont...\n","2          3      0                                     bihday majesti\n","3          4      0                        model love u take u time ur\n","4          5      0                            factsguid societi motiv\n","...      ...    ...                                                ...\n","31957  31958      0                                 ate user isz youuu\n","31958  31959      0  see nina turner airwav tri wrap mantl genuin h...\n","31959  31960      0           listen sad song monday morn otw work sad\n","31960  31961      1   user sikh templ vandalis calgari wso condemn act\n","31961  31962      0                                  thank user follow\n","\n","[31962 rows x 3 columns]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Lemmatizing\n","\n","dataset['tweet']=dataset['tweet'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"y7kv2wdS8Hpa"},"source":["### `SPLITTING DATA FOR TRAINING AND TESTING`"]},{"cell_type":"markdown","metadata":{"id":"FpItdfR28Hpd"},"source":["#### `LSTM MODEL`"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before Tokenization & Padding \n"," user father dysfunct selfish drag kid dysfunct run\n","After Tokenization & Padding \n"," [   1   18 2514 1821  129  193    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n"]}],"source":["max_words = 1000\n","max_len=50\n","\n","def tokenize_pad_sequences(text):\n","    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n","    tokenizer.fit_on_texts(text)\n","    X = tokenizer.texts_to_sequences(text)\n","    X = pad_sequences(X, padding='post', maxlen=max_len)\n","    return X, tokenizer\n","\n","print('Before Tokenization & Padding \\n', dataset['tweet'][0])\n","X, tokenizer = tokenize_pad_sequences(dataset['tweet'])\n","print('After Tokenization & Padding \\n', X[0])"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 50, 128)           640000    \n","                                                                 \n"," spatial_dropout1d (SpatialD  (None, 50, 128)          0         \n"," ropout1D)                                                       \n","                                                                 \n"," lstm (LSTM)                 (None, 196)               254800    \n","                                                                 \n"," dense (Dense)               (None, 2)                 394       \n","                                                                 \n","=================================================================\n","Total params: 895,194\n","Trainable params: 895,194\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["embed_dim = 128\n","lstm_out = 196\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embed_dim,input_length = X.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(25569, 50) (25569, 2)\n","(6393, 50) (6393, 2)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","Y=pd.get_dummies(dataset[\"label\"]).values\n","X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n","print(X_train.shape,y_train.shape)\n","print(X_test.shape,y_test.shape)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","800/800 - 55s - loss: 0.2332 - accuracy: 0.9301 - 55s/epoch - 69ms/step\n","Epoch 2/7\n","800/800 - 55s - loss: 0.2336 - accuracy: 0.9301 - 55s/epoch - 68ms/step\n","Epoch 3/7\n","800/800 - 56s - loss: 0.2512 - accuracy: 0.9301 - 56s/epoch - 70ms/step\n","Epoch 4/7\n","800/800 - 54s - loss: 0.2541 - accuracy: 0.9301 - 54s/epoch - 68ms/step\n","Epoch 5/7\n","800/800 - 54s - loss: 0.2519 - accuracy: 0.9301 - 54s/epoch - 68ms/step\n","Epoch 6/7\n","800/800 - 54s - loss: 0.2485 - accuracy: 0.9301 - 54s/epoch - 67ms/step\n","Epoch 7/7\n","800/800 - 53s - loss: 0.2437 - accuracy: 0.9301 - 53s/epoch - 67ms/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1ff86230ee0>"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = 32\n","model.fit(X_train, y_train, epochs = 7, batch_size=batch_size, verbose = 2)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","1/1 - 4s - 4s/epoch - 4s/step\n","negative\n"]}],"source":["twt = [[\"good day\"]]\n","#vectorizing the tweet by the pre-fitted tokenizer instance\n","twt = tokenizer.texts_to_sequences(twt)\n","#padding the tweet to have exactly the same shape as `embedding_2` input\n","twt = pad_sequences(twt, maxlen=50, dtype='int32', value=0)\n","print(twt)\n","sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n","if(np.argmax(sentiment) == 0):\n","    print(\"negative\")\n","elif (np.argmax(sentiment) == 1):\n","    print(\"positive\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ml.ipynb","provenance":[]},"interpreter":{"hash":"9914e116a5374b2bff9620cffda55a18bf7ad1aca0b02877c92a777ba6978604"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
