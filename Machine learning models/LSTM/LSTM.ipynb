{"cells":[{"cell_type":"markdown","metadata":{"id":"ePoUkERg8HpR"},"source":["# `SENTIMENTAL ANALYSIS`"]},{"cell_type":"markdown","metadata":{"id":"QXl6zLTm8HpU"},"source":["#### `IMPORTIN LIBRARIES`"]},{"cell_type":"code","execution_count":279,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1650258543867,"user":{"displayName":"Ganesh Shinde","userId":"04780742815385189197"},"user_tz":-330},"id":"hFs4rEpz8HpV"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","import pickle\n","import string\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D"]},{"cell_type":"markdown","metadata":{"id":"cwJdfYrs8HpX"},"source":["### `IMPORTING DATASET`"]},{"cell_type":"code","execution_count":280,"metadata":{},"outputs":[],"source":["dataset=pd.read_csv(\"train.csv\")"]},{"cell_type":"code","execution_count":281,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>@user when a father is dysfunctional and is s...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>@user @user thanks for #lyft credit i can't us...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday your majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>#model   i love u take with u all the time in ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide: society now    #motivation</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>[2/2] huge fan fare and big talking before the...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>@user camping tomorrow @user @user @user @use...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>the next school year is the year for exams.ð...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>we won!!! love the land!!! #allin #cavs #champ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>@user @user welcome here !  i'm   it's so #gr...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>â #ireland consumer price index (mom) climb...</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>we are so selfish. #orlando #standwithorlando ...</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>i get to see my daddy today!!   #80days #getti...</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>@user #cnn calls #michigan middle school 'buil...</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>1</td>\n","      <td>no comment!  in #australia   #opkillingbay #se...</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>0</td>\n","      <td>ouch...junior is angryð#got7 #junior #yugyo...</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>i am thankful for having a paner. #thankful #p...</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>1</td>\n","      <td>retweet if you agree!</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>its #friday! ð smiles all around via ig use...</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>0</td>\n","      <td>as we all know, essential oils are not made of...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id  label                                              tweet\n","0    1      0   @user when a father is dysfunctional and is s...\n","1    2      0  @user @user thanks for #lyft credit i can't us...\n","2    3      0                                bihday your majesty\n","3    4      0  #model   i love u take with u all the time in ...\n","4    5      0             factsguide: society now    #motivation\n","5    6      0  [2/2] huge fan fare and big talking before the...\n","6    7      0   @user camping tomorrow @user @user @user @use...\n","7    8      0  the next school year is the year for exams.ð...\n","8    9      0  we won!!! love the land!!! #allin #cavs #champ...\n","9   10      0   @user @user welcome here !  i'm   it's so #gr...\n","10  11      0   â #ireland consumer price index (mom) climb...\n","11  12      0  we are so selfish. #orlando #standwithorlando ...\n","12  13      0  i get to see my daddy today!!   #80days #getti...\n","13  14      1  @user #cnn calls #michigan middle school 'buil...\n","14  15      1  no comment!  in #australia   #opkillingbay #se...\n","15  16      0  ouch...junior is angryð#got7 #junior #yugyo...\n","16  17      0  i am thankful for having a paner. #thankful #p...\n","17  18      1                             retweet if you agree! \n","18  19      0  its #friday! ð smiles all around via ig use...\n","19  20      0  as we all know, essential oils are not made of..."]},"execution_count":281,"metadata":{},"output_type":"execute_result"}],"source":["dataset.head(20)"]},{"cell_type":"markdown","metadata":{"id":"Cc_iEAkH8HpY"},"source":["### `PRE-PROCESSING`"]},{"cell_type":"markdown","metadata":{"id":"uw8hfhNc9JmN"},"source":["##### `PRE PROCESSING VARIABLE SETTING`"]},{"cell_type":"code","execution_count":282,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1650258545583,"user":{"displayName":"Ganesh Shinde","userId":"04780742815385189197"},"user_tz":-330},"id":"j_sxAkOp9roT"},"outputs":[],"source":["STOP_WORDS=stopwords.words('english')\n","TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\""]},{"cell_type":"code","execution_count":283,"metadata":{},"outputs":[],"source":["dataset['tweet'] = dataset['tweet'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"]},{"cell_type":"code","execution_count":284,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>user father dysfunctional selfish drags kids d...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>user user thanks lyft credit cant use cause do...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>bihday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>model love u take u time ur</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>31958</td>\n","      <td>0</td>\n","      <td>ate user isz youuu</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>31959</td>\n","      <td>0</td>\n","      <td>see nina turner airwaves trying wrap mantle ge...</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>31960</td>\n","      <td>0</td>\n","      <td>listening sad songs monday morning otw work sad</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>31961</td>\n","      <td>1</td>\n","      <td>user sikh temple vandalised calgary wso condem...</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>31962</td>\n","      <td>0</td>\n","      <td>thank user follow</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 3 columns</p>\n","</div>"],"text/plain":["          id  label                                              tweet\n","0          1      0  user father dysfunctional selfish drags kids d...\n","1          2      0  user user thanks lyft credit cant use cause do...\n","2          3      0                                     bihday majesty\n","3          4      0                        model love u take u time ur\n","4          5      0                      factsguide society motivation\n","...      ...    ...                                                ...\n","31957  31958      0                                 ate user isz youuu\n","31958  31959      0  see nina turner airwaves trying wrap mantle ge...\n","31959  31960      0    listening sad songs monday morning otw work sad\n","31960  31961      1  user sikh temple vandalised calgary wso condem...\n","31961  31962      0                                  thank user follow\n","\n","[31962 rows x 3 columns]"]},"execution_count":284,"metadata":{},"output_type":"execute_result"}],"source":["# removing stop words\n","dataset.tweet=dataset.tweet.apply(lambda x : \" \".join (x for x in x.split() if x not in STOP_WORDS))\n","dataset"]},{"cell_type":"code","execution_count":287,"metadata":{},"outputs":[{"data":{"text/plain":["0    user father dysfunct selfish drag kid dysfunct...\n","1    user user thank lyft credit cant use caus dont...\n","2                                       bihday majesti\n","3                          model love u take u time ur\n","4                              factsguid societi motiv\n","Name: tweet, dtype: object"]},"execution_count":287,"metadata":{},"output_type":"execute_result"}],"source":["# Stemming\n","st=PorterStemmer()\n","dataset['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"]},{"cell_type":"code","execution_count":288,"metadata":{},"outputs":[{"data":{"text/plain":["0        user father dysfunctional selfish drag kid dys...\n","1        user user thanks lyft credit cant use cause do...\n","2                                           bihday majesty\n","3                              model love u take u time ur\n","4                            factsguide society motivation\n","                               ...                        \n","31957                                   ate user isz youuu\n","31958    see nina turner airwave trying wrap mantle gen...\n","31959       listening sad song monday morning otw work sad\n","31960    user sikh temple vandalised calgary wso condem...\n","31961                                    thank user follow\n","Name: tweet, Length: 31962, dtype: object"]},"execution_count":288,"metadata":{},"output_type":"execute_result"}],"source":["# Lemmatizing\n","\n","dataset['tweet']=dataset['tweet'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n","dataset['tweet']"]},{"cell_type":"code","execution_count":289,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>user father dysfunctional selfish drag kid dys...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>user user thanks lyft credit cant use cause do...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>bihday majesty</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>model love u take u time ur</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>factsguide society motivation</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>0</td>\n","      <td>ate user isz youuu</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>0</td>\n","      <td>see nina turner airwave trying wrap mantle gen...</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>0</td>\n","      <td>listening sad song monday morning otw work sad</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>1</td>\n","      <td>user sikh temple vandalised calgary wso condem...</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>0</td>\n","      <td>thank user follow</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 2 columns</p>\n","</div>"],"text/plain":["       label                                              tweet\n","0          0  user father dysfunctional selfish drag kid dys...\n","1          0  user user thanks lyft credit cant use cause do...\n","2          0                                     bihday majesty\n","3          0                        model love u take u time ur\n","4          0                      factsguide society motivation\n","...      ...                                                ...\n","31957      0                                 ate user isz youuu\n","31958      0  see nina turner airwave trying wrap mantle gen...\n","31959      0     listening sad song monday morning otw work sad\n","31960      1  user sikh temple vandalised calgary wso condem...\n","31961      0                                  thank user follow\n","\n","[31962 rows x 2 columns]"]},"execution_count":289,"metadata":{},"output_type":"execute_result"}],"source":["dataset=dataset.drop([\"id\"],axis=1)\n","dataset"]},{"cell_type":"code","execution_count":290,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>user father dysfunctional selfish drag kid dys...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>user user thanks lyft credit cant use cause do...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>bihday majesty</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>model love u take u time ur</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>factsguide society motivation</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>31957</th>\n","      <td>ate user isz youuu</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31958</th>\n","      <td>see nina turner airwave trying wrap mantle gen...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31959</th>\n","      <td>listening sad song monday morning otw work sad</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>31960</th>\n","      <td>user sikh temple vandalised calgary wso condem...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>31961</th>\n","      <td>thank user follow</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>31962 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                   label  tweet\n","0      user father dysfunctional selfish drag kid dys...      0\n","1      user user thanks lyft credit cant use cause do...      0\n","2                                         bihday majesty      0\n","3                            model love u take u time ur      0\n","4                          factsguide society motivation      0\n","...                                                  ...    ...\n","31957                                 ate user isz youuu      0\n","31958  see nina turner airwave trying wrap mantle gen...      0\n","31959     listening sad song monday morning otw work sad      0\n","31960  user sikh temple vandalised calgary wso condem...      1\n","31961                                  thank user follow      0\n","\n","[31962 rows x 2 columns]"]},"execution_count":290,"metadata":{},"output_type":"execute_result"}],"source":["col_list=list(dataset.columns)\n","dataset[\"label\"],dataset[\"tweet\"]=dataset[\"tweet\"],dataset[\"label\"]\n","dataset.columns=col_list\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"y7kv2wdS8Hpa"},"source":["### `SPLITTING DATA FOR TRAINING AND TESTING`"]},{"cell_type":"markdown","metadata":{"id":"FpItdfR28Hpd"},"source":["#### `LSTM MODEL`"]},{"cell_type":"code","execution_count":294,"metadata":{},"outputs":[],"source":["X=dataset.tweet\n","Y=dataset.label"]},{"cell_type":"code","execution_count":295,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=0)"]},{"cell_type":"code","execution_count":296,"metadata":{},"outputs":[],"source":["def tokenizing_padding(x):\n","    max_fatures = 2000\n","    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n","    tokenizer.fit_on_texts(dataset['text'].values)\n","    X = tokenizer.texts_to_sequences(dataset['text'].values)\n","    X = pad_sequences(X)\n","    return X,tokenizer\n"]},{"cell_type":"code","execution_count":297,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"'int' object is not iterable","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32mc:\\Users\\ganes\\OneDrive\\Documents\\GitHub\\EDI-sem4-project\\Machine learning models\\LSTM\\LSTM.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=0'>1</a>\u001b[0m vocabulary_size \u001b[39m=\u001b[39m \u001b[39m5000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=1'>2</a>\u001b[0m count_vector \u001b[39m=\u001b[39m CountVectorizer(max_features\u001b[39m=\u001b[39mvocabulary_size,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=2'>3</a>\u001b[0m                                 preprocessor\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=3'>4</a>\u001b[0m                                tokenizer\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x) \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=4'>5</a>\u001b[0m X_train \u001b[39m=\u001b[39m count_vector\u001b[39m.\u001b[39;49mfit_transform(X_train)\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ganes/OneDrive/Documents/GitHub/EDI-sem4-project/Machine%20learning%20models/LSTM/LSTM.ipynb#ch0000030?line=5'>6</a>\u001b[0m X_test \u001b[39m=\u001b[39m count_vector\u001b[39m.\u001b[39mtransform(X_test)\u001b[39m.\u001b[39mtoarray()\n","File \u001b[1;32mc:\\Users\\ganes\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1330'>1331</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1333'>1334</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1334'>1335</a>\u001b[0m             )\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1335'>1336</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1337'>1338</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1339'>1340</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1340'>1341</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[1;32mc:\\Users\\ganes\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1206'>1207</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1207'>1208</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1208'>1209</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1209'>1210</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/ganes/AppData/Local/Programs/Python/Python39/lib/site-packages/sklearn/feature_extraction/text.py?line=1210'>1211</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n","\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"]}],"source":["vocabulary_size = 5000\n","count_vector = CountVectorizer(max_features=vocabulary_size,\n","                                preprocessor=lambda x: x,\n","                               tokenizer=lambda x: x) \n","X_train = count_vector.fit_transform(X_train).toarray()\n","X_test = count_vector.transform(X_test).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=0)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ml.ipynb","provenance":[]},"interpreter":{"hash":"9914e116a5374b2bff9620cffda55a18bf7ad1aca0b02877c92a777ba6978604"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
