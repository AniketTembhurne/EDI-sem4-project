{"cells":[{"cell_type":"markdown","metadata":{"id":"ePoUkERg8HpR"},"source":["# `SENTIMENTAL ANALYSIS`"]},{"cell_type":"markdown","metadata":{"id":"QXl6zLTm8HpU"},"source":["#### `IMPORTIN LIBRARIES`"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1650258543867,"user":{"displayName":"Ganesh Shinde","userId":"04780742815385189197"},"user_tz":-330},"id":"hFs4rEpz8HpV"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n","from keras.metrics import Precision, Recall\n","from tensorflow.keras.optimizers import SGD\n","import keras.backend as K\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{"id":"cwJdfYrs8HpX"},"source":["### `IMPORTING DATASET`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ds=pd.read_csv(\"tweet_train.csv\",encoding=\"ISO-8859-1\",names=[\"label\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"])\n","ds=ds.drop([\"ids\",\"date\",\"flag\",\"user\"],axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["positive=ds[ds[\"label\"]==0][:10000]\n","neagtive=ds[ds[\"label\"]==4][:10000]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["positive.info()\n","print()\n","neagtive.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["positive"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["neagtive[\"label\"]=neagtive[\"label\"].apply(lambda x:1)\n","neagtive"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset=pd.concat([positive,neagtive])\n","dataset=dataset.sample(frac=1)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"Cc_iEAkH8HpY"},"source":["### `PRE-PROCESSING`"]},{"cell_type":"markdown","metadata":{"id":"uw8hfhNc9JmN"},"source":["##### `PRE PROCESSING VARIABLE SETTING`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dataset['tweet'] = dataset['tweet'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n","dataset['tweet'] = dataset['tweet'].apply(lambda x: re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\",' ',x))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset[\"tweet\"]=dataset[\"tweet\"].apply(lambda x: \" \".join([w for w in x.split() if w not in stopwords.words(\"english\")]))\n","dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Stemming\n","st=PorterStemmer()\n","dataset[\"tweet\"]=dataset['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Lemmatizing\n","\n","dataset['tweet']=dataset['tweet'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"y7kv2wdS8Hpa"},"source":["### `SPLITTING DATA FOR TRAINING AND TESTING`"]},{"cell_type":"markdown","metadata":{"id":"FpItdfR28Hpd"},"source":["#### `LSTM MODEL`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_words = 1000\n","max_len=50\n","\n","def tokenize_pad_sequences(text):\n","    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n","    tokenizer.fit_on_texts(text)\n","    X = tokenizer.texts_to_sequences(text)\n","    X = pad_sequences(X, padding='post', maxlen=max_len)\n","    return X, tokenizer\n","\n","print('Before Tokenization & Padding \\n', dataset['tweet'][0])\n","X, tokenizer = tokenize_pad_sequences(dataset['tweet'])\n","print('After Tokenization & Padding \\n', X[0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vocab_size = 5000\n","embedding_size = 32\n","epochs=20\n","learning_rate = 0.1\n","decay_rate = learning_rate / epochs\n","momentum = 0.8\n","\n","sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n","model= Sequential()\n","model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Bidirectional(tf.compat.v1.keras.layers.CuDNNLSTM(32)))\n","model.add(Dropout(0.2))\n","model.add(Dense(2, activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","Y=pd.get_dummies(dataset[\"label\"]).values\n","X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n","print(X_train.shape,y_train.shape)\n","print(X_test.shape,y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def f1_score(precision, recall):\n","    \n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(model.summary())\n","\n","model.compile(loss='binary_crossentropy', optimizer=sgd, \n","               metrics=['accuracy', Precision(), Recall()])\n","\n","\n","history = model.fit(X_train, y_train,\n","                      validation_data=(X_test, y_test),\n","                      batch_size=10, epochs=30, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n","print('')\n","print('Accuracy  : {:.4f}'.format(accuracy))\n","print('Precision : {:.4f}'.format(precision))\n","print('Recall    : {:.4f}'.format(recall))\n","print('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["twt = [\" fucking bitch\"]\n","#vectorizing the tweet by the pre-fitted tokenizer instance\n","twt = tokenizer.texts_to_sequences(twt)\n","#padding the tweet to have exactly the same shape as `embedding_2` input\n","twt = pad_sequences(twt, maxlen=50, dtype='int32', value=0)\n","print(twt)\n","sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n","if(np.argmax(sentiment) == 0):\n","    print(\"negative\")\n","elif (np.argmax(sentiment) == 1):\n","    print(\"positive\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ml.ipynb","provenance":[]},"interpreter":{"hash":"9914e116a5374b2bff9620cffda55a18bf7ad1aca0b02877c92a777ba6978604"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
