{"cells":[{"cell_type":"markdown","metadata":{"id":"ePoUkERg8HpR"},"source":["# `SENTIMENTAL ANALYSIS`"]},{"cell_type":"markdown","metadata":{"id":"QXl6zLTm8HpU"},"source":["#### `IMPORTIN LIBRARIES`"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1650258543867,"user":{"displayName":"Ganesh Shinde","userId":"04780742815385189197"},"user_tz":-330},"id":"hFs4rEpz8HpV"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\ganes\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import re\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import word_tokenize\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('punkt')\n","from textblob import Word\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","from keras.models import Sequential\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.optimizers import SGD"]},{"cell_type":"markdown","metadata":{"id":"cwJdfYrs8HpX"},"source":["### `IMPORTING DATASET`"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["ds=pd.read_csv(\"tweet_train.csv\",encoding=\"ISO-8859-1\",names=[\"label\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"])\n","ds=ds.drop([\"ids\",\"date\",\"flag\",\"user\"],axis=1)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["positive=ds[ds[\"label\"]==0][:10000]\n","neagtive=ds[ds[\"label\"]==4][:10000]"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 10000 entries, 0 to 9999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   label   10000 non-null  int64 \n"," 1   tweet   10000 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 234.4+ KB\n","\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 10000 entries, 800000 to 809999\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   label   10000 non-null  int64 \n"," 1   tweet   10000 non-null  object\n","dtypes: int64(1), object(1)\n","memory usage: 234.4+ KB\n"]}],"source":["positive.info()\n","print()\n","neagtive.info()"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>0</td>\n","      <td>Aww that's sad</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>0</td>\n","      <td>stupid dvds stuffing up the good bits in jaws.</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>0</td>\n","      <td>@Dandy_Sephy No. Only close friends and family...</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>0</td>\n","      <td>CRAP! After looking when I last tweeted... WHY...</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>0</td>\n","      <td>Its Another Rainboot day</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>"],"text/plain":["      label                                              tweet\n","0         0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1         0  is upset that he can't update his Facebook by ...\n","2         0  @Kenichan I dived many times for the ball. Man...\n","3         0    my whole body feels itchy and like its on fire \n","4         0  @nationwideclass no, it's not behaving at all....\n","...     ...                                                ...\n","9995      0                                    Aww that's sad \n","9996      0    stupid dvds stuffing up the good bits in jaws. \n","9997      0  @Dandy_Sephy No. Only close friends and family...\n","9998      0  CRAP! After looking when I last tweeted... WHY...\n","9999      0                          Its Another Rainboot day \n","\n","[10000 rows x 2 columns]"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["positive"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>800000</th>\n","      <td>1</td>\n","      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n","    </tr>\n","    <tr>\n","      <th>800001</th>\n","      <td>1</td>\n","      <td>im meeting up with one of my besties tonight! ...</td>\n","    </tr>\n","    <tr>\n","      <th>800002</th>\n","      <td>1</td>\n","      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n","    </tr>\n","    <tr>\n","      <th>800003</th>\n","      <td>1</td>\n","      <td>Being sick can be really cheap when it hurts t...</td>\n","    </tr>\n","    <tr>\n","      <th>800004</th>\n","      <td>1</td>\n","      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>809995</th>\n","      <td>1</td>\n","      <td>Morning! I have slacked for two days in twitte...</td>\n","    </tr>\n","    <tr>\n","      <th>809996</th>\n","      <td>1</td>\n","      <td>@bensummers Isn't that sweet of them.... Altru...</td>\n","    </tr>\n","    <tr>\n","      <th>809997</th>\n","      <td>1</td>\n","      <td>@jakrose Um, milk *fathers* don't have udders....</td>\n","    </tr>\n","    <tr>\n","      <th>809998</th>\n","      <td>1</td>\n","      <td>@zenaweist They could also tweet @BeccaRoberts</td>\n","    </tr>\n","    <tr>\n","      <th>809999</th>\n","      <td>1</td>\n","      <td>Good lord, I still have 125 work emails to cat...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                              tweet\n","800000      1       I LOVE @Health4UandPets u guys r the best!! \n","800001      1  im meeting up with one of my besties tonight! ...\n","800002      1  @DaRealSunisaKim Thanks for the Twitter add, S...\n","800003      1  Being sick can be really cheap when it hurts t...\n","800004      1    @LovesBrooklyn2 he has that effect on everyone \n","...       ...                                                ...\n","809995      1  Morning! I have slacked for two days in twitte...\n","809996      1  @bensummers Isn't that sweet of them.... Altru...\n","809997      1  @jakrose Um, milk *fathers* don't have udders....\n","809998      1    @zenaweist They could also tweet @BeccaRoberts \n","809999      1  Good lord, I still have 125 work emails to cat...\n","\n","[10000 rows x 2 columns]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["neagtive[\"label\"]=neagtive[\"label\"].apply(lambda x:1)\n","neagtive"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5313</th>\n","      <td>0</td>\n","      <td>I'm not excited for the yearly Holy Week trip ...</td>\n","    </tr>\n","    <tr>\n","      <th>804190</th>\n","      <td>1</td>\n","      <td>is starting her day of laundry and cleaning.. ...</td>\n","    </tr>\n","    <tr>\n","      <th>7592</th>\n","      <td>0</td>\n","      <td>@loqenz_niteowl I'm glad she got there safe! M...</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>0</td>\n","      <td>I wish I was better at writing. It's taking me...</td>\n","    </tr>\n","    <tr>\n","      <th>804384</th>\n","      <td>1</td>\n","      <td>Ooh, need a TV! @twitchhiker is on This Mornin...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>803016</th>\n","      <td>1</td>\n","      <td>@RadarDog grass??? And it does not make them s...</td>\n","    </tr>\n","    <tr>\n","      <th>4646</th>\n","      <td>0</td>\n","      <td>@ravogd a spider? can it be covered when im ro...</td>\n","    </tr>\n","    <tr>\n","      <th>802688</th>\n","      <td>1</td>\n","      <td>Funny kid gives the evil look - Pretty damn fu...</td>\n","    </tr>\n","    <tr>\n","      <th>2628</th>\n","      <td>0</td>\n","      <td>hungry  dreaming of yumyums..</td>\n","    </tr>\n","    <tr>\n","      <th>7223</th>\n","      <td>0</td>\n","      <td>@eyesbehindshade yo homie!!! whats nyc saying?...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                              tweet\n","5313        0  I'm not excited for the yearly Holy Week trip ...\n","804190      1  is starting her day of laundry and cleaning.. ...\n","7592        0  @loqenz_niteowl I'm glad she got there safe! M...\n","326         0  I wish I was better at writing. It's taking me...\n","804384      1  Ooh, need a TV! @twitchhiker is on This Mornin...\n","...       ...                                                ...\n","803016      1  @RadarDog grass??? And it does not make them s...\n","4646        0  @ravogd a spider? can it be covered when im ro...\n","802688      1  Funny kid gives the evil look - Pretty damn fu...\n","2628        0                      hungry  dreaming of yumyums..\n","7223        0  @eyesbehindshade yo homie!!! whats nyc saying?...\n","\n","[20000 rows x 2 columns]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["dataset=pd.concat([positive,neagtive])\n","dataset=dataset.sample(frac=1)\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"Cc_iEAkH8HpY"},"source":["### `PRE-PROCESSING`"]},{"cell_type":"markdown","metadata":{"id":"uw8hfhNc9JmN"},"source":["##### `PRE PROCESSING VARIABLE SETTING`"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# dataset['tweet'] = dataset['tweet'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n","dataset['tweet'] = dataset['tweet'].apply(lambda x: re.sub(\"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\",' ',x))"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5313</th>\n","      <td>0</td>\n","      <td>I m not excited for the yearly Holy Week trip ...</td>\n","    </tr>\n","    <tr>\n","      <th>804190</th>\n","      <td>1</td>\n","      <td>is starting her day of laundry and cleaning le...</td>\n","    </tr>\n","    <tr>\n","      <th>7592</th>\n","      <td>0</td>\n","      <td>I m glad she got there safe Miss her already</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>0</td>\n","      <td>I wish I was better at writing It s taking me ...</td>\n","    </tr>\n","    <tr>\n","      <th>804384</th>\n","      <td>1</td>\n","      <td>Ooh need a TV twitchhiker is on This Morning a...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>803016</th>\n","      <td>1</td>\n","      <td>grass And it does not make them sick I would...</td>\n","    </tr>\n","    <tr>\n","      <th>4646</th>\n","      <td>0</td>\n","      <td>a spider can it be covered when im round sli...</td>\n","    </tr>\n","    <tr>\n","      <th>802688</th>\n","      <td>1</td>\n","      <td>Funny kid gives the evil look Pretty damn funny</td>\n","    </tr>\n","    <tr>\n","      <th>2628</th>\n","      <td>0</td>\n","      <td>hungry dreaming of yumyums</td>\n","    </tr>\n","    <tr>\n","      <th>7223</th>\n","      <td>0</td>\n","      <td>yo homie whats nyc saying Gonna miss you thi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                              tweet\n","5313        0  I m not excited for the yearly Holy Week trip ...\n","804190      1  is starting her day of laundry and cleaning le...\n","7592        0      I m glad she got there safe Miss her already \n","326         0  I wish I was better at writing It s taking me ...\n","804384      1  Ooh need a TV twitchhiker is on This Morning a...\n","...       ...                                                ...\n","803016      1    grass And it does not make them sick I would...\n","4646        0    a spider can it be covered when im round sli...\n","802688      1  Funny kid gives the evil look Pretty damn funny  \n","2628        0                        hungry dreaming of yumyums \n","7223        0    yo homie whats nyc saying Gonna miss you thi...\n","\n","[20000 rows x 2 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5313</th>\n","      <td>0</td>\n","      <td>I excited yearly Holy Week trip beach</td>\n","    </tr>\n","    <tr>\n","      <th>804190</th>\n","      <td>1</td>\n","      <td>starting day laundry cleaning leaving Bath T 2...</td>\n","    </tr>\n","    <tr>\n","      <th>7592</th>\n","      <td>0</td>\n","      <td>I glad got safe Miss already</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>0</td>\n","      <td>I wish I better writing It taking long write p...</td>\n","    </tr>\n","    <tr>\n","      <th>804384</th>\n","      <td>1</td>\n","      <td>Ooh need TV twitchhiker This Morning 11ish Big...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>803016</th>\n","      <td>1</td>\n","      <td>grass And make sick I would like two leg picki...</td>\n","    </tr>\n","    <tr>\n","      <th>4646</th>\n","      <td>0</td>\n","      <td>spider covered im round slight phobia</td>\n","    </tr>\n","    <tr>\n","      <th>802688</th>\n","      <td>1</td>\n","      <td>Funny kid gives evil look Pretty damn funny</td>\n","    </tr>\n","    <tr>\n","      <th>2628</th>\n","      <td>0</td>\n","      <td>hungry dreaming yumyums</td>\n","    </tr>\n","    <tr>\n","      <th>7223</th>\n","      <td>0</td>\n","      <td>yo homie whats nyc saying Gonna miss weekend</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                              tweet\n","5313        0              I excited yearly Holy Week trip beach\n","804190      1  starting day laundry cleaning leaving Bath T 2...\n","7592        0                       I glad got safe Miss already\n","326         0  I wish I better writing It taking long write p...\n","804384      1  Ooh need TV twitchhiker This Morning 11ish Big...\n","...       ...                                                ...\n","803016      1  grass And make sick I would like two leg picki...\n","4646        0              spider covered im round slight phobia\n","802688      1        Funny kid gives evil look Pretty damn funny\n","2628        0                            hungry dreaming yumyums\n","7223        0       yo homie whats nyc saying Gonna miss weekend\n","\n","[20000 rows x 2 columns]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"tweet\"]=dataset[\"tweet\"].apply(lambda x: \" \".join([w for w in x.split() if w not in stopwords.words(\"english\")]))\n","dataset"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# Stemming\n","st=PorterStemmer()\n","dataset[\"tweet\"]=dataset['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5313</th>\n","      <td>0</td>\n","      <td>i excit yearli holi week trip beach</td>\n","    </tr>\n","    <tr>\n","      <th>804190</th>\n","      <td>1</td>\n","      <td>start day laundri clean leav bath t 20 hour al...</td>\n","    </tr>\n","    <tr>\n","      <th>7592</th>\n","      <td>0</td>\n","      <td>i glad got safe miss alreadi</td>\n","    </tr>\n","    <tr>\n","      <th>326</th>\n","      <td>0</td>\n","      <td>i wish i better write it take long write paper</td>\n","    </tr>\n","    <tr>\n","      <th>804384</th>\n","      <td>1</td>\n","      <td>ooh need tv twitchhik thi morn 11ish big stuff...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>803016</th>\n","      <td>1</td>\n","      <td>grass and make sick i would like two leg pick ...</td>\n","    </tr>\n","    <tr>\n","      <th>4646</th>\n","      <td>0</td>\n","      <td>spider cover im round slight phobia</td>\n","    </tr>\n","    <tr>\n","      <th>802688</th>\n","      <td>1</td>\n","      <td>funni kid give evil look pretti damn funni</td>\n","    </tr>\n","    <tr>\n","      <th>2628</th>\n","      <td>0</td>\n","      <td>hungri dream yumyum</td>\n","    </tr>\n","    <tr>\n","      <th>7223</th>\n","      <td>0</td>\n","      <td>yo homi what nyc say gonna miss weekend</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20000 rows × 2 columns</p>\n","</div>"],"text/plain":["        label                                              tweet\n","5313        0                i excit yearli holi week trip beach\n","804190      1  start day laundri clean leav bath t 20 hour al...\n","7592        0                       i glad got safe miss alreadi\n","326         0     i wish i better write it take long write paper\n","804384      1  ooh need tv twitchhik thi morn 11ish big stuff...\n","...       ...                                                ...\n","803016      1  grass and make sick i would like two leg pick ...\n","4646        0                spider cover im round slight phobia\n","802688      1         funni kid give evil look pretti damn funni\n","2628        0                                hungri dream yumyum\n","7223        0            yo homi what nyc say gonna miss weekend\n","\n","[20000 rows x 2 columns]"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# Lemmatizing\n","\n","dataset['tweet']=dataset['tweet'].apply(lambda x:\" \".join([Word(word).lemmatize() for word in x.split()]))\n","dataset"]},{"cell_type":"markdown","metadata":{"id":"y7kv2wdS8Hpa"},"source":["### `SPLITTING DATA FOR TRAINING AND TESTING`"]},{"cell_type":"markdown","metadata":{"id":"FpItdfR28Hpd"},"source":["#### `LSTM MODEL`"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before Tokenization & Padding \n"," awww bummer you shoulda got david carr third day d\n","After Tokenization & Padding \n"," [  1 177  57 419 568   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"]}],"source":["max_words = 1000\n","max_len=50\n","\n","def tokenize_pad_sequences(text):\n","    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n","    tokenizer.fit_on_texts(text)\n","    X = tokenizer.texts_to_sequences(text)\n","    X = pad_sequences(X, padding='post', maxlen=max_len)\n","    return X, tokenizer\n","\n","print('Before Tokenization & Padding \\n', dataset['tweet'][0])\n","X, tokenizer = tokenize_pad_sequences(dataset['tweet'])\n","print('After Tokenization & Padding \\n', X[0])"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 50, 128)           128000    \n","                                                                 \n"," spatial_dropout1d (SpatialD  (None, 50, 128)          0         \n"," ropout1D)                                                       \n","                                                                 \n"," lstm (LSTM)                 (None, 196)               254800    \n","                                                                 \n"," dense (Dense)               (None, 2)                 394       \n","                                                                 \n","=================================================================\n","Total params: 383,194\n","Trainable params: 383,194\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["embed_dim = 128\n","lstm_out = 196\n","\n","model = Sequential()\n","model.add(Embedding(max_words, embed_dim,input_length = X.shape[1]))\n","model.add(SpatialDropout1D(0.4))\n","model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(2,activation='softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n","print(model.summary())"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(16000, 50) (16000, 2)\n","(4000, 50) (4000, 2)\n"]}],"source":["from sklearn.model_selection import train_test_split\n","Y=pd.get_dummies(dataset[\"label\"]).values\n","X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\n","print(X_train.shape,y_train.shape)\n","print(X_test.shape,y_test.shape)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/7\n","500/500 - 61s - loss: 0.6941 - accuracy: 0.5011 - 61s/epoch - 122ms/step\n","Epoch 2/7\n","500/500 - 48s - loss: 0.6934 - accuracy: 0.5042 - 48s/epoch - 96ms/step\n","Epoch 3/7\n","500/500 - 48s - loss: 0.6934 - accuracy: 0.4983 - 48s/epoch - 96ms/step\n","Epoch 4/7\n","500/500 - 61s - loss: 0.6934 - accuracy: 0.5041 - 61s/epoch - 121ms/step\n","Epoch 5/7\n","500/500 - 67s - loss: 0.6933 - accuracy: 0.4972 - 67s/epoch - 134ms/step\n","Epoch 6/7\n","500/500 - 59s - loss: 0.6934 - accuracy: 0.4911 - 59s/epoch - 118ms/step\n","Epoch 7/7\n","500/500 - 71s - loss: 0.6933 - accuracy: 0.4924 - 71s/epoch - 142ms/step\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x21032379f70>"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["batch_size = 32\n","model.fit(X_train, y_train, epochs = 7, batch_size=batch_size, verbose = 2)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","1/1 - 0s - 34ms/epoch - 34ms/step\n","positive\n"]}],"source":["twt = [\"\"]\n","#vectorizing the tweet by the pre-fitted tokenizer instance\n","twt = tokenizer.texts_to_sequences(twt)\n","#padding the tweet to have exactly the same shape as `embedding_2` input\n","twt = pad_sequences(twt, maxlen=50, dtype='int32', value=0)\n","print(twt)\n","sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n","if(np.argmax(sentiment) == 0):\n","    print(\"negative\")\n","elif (np.argmax(sentiment) == 1):\n","    print(\"positive\")"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["125/125 [==============================] - 6s 27ms/step - loss: 0.6932 - accuracy: 0.4988\n"]},{"data":{"text/plain":["[0.6931515336036682, 0.4987500011920929]"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["model.evaluate(X_test,y_test)"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[  1  77 173 483 173 312 189   1 101  20 483   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0] 1\n"]}],"source":["print(X_test[0],y_test[0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"ml.ipynb","provenance":[]},"interpreter":{"hash":"9914e116a5374b2bff9620cffda55a18bf7ad1aca0b02877c92a777ba6978604"},"kernelspec":{"display_name":"Python 3.9.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
